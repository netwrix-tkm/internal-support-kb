"Id","ParentId","Type","CreatedById","CreatedDate","Body","Title","LinkUrl","RelatedRecordId","InsertedById","LastModifiedDate"
"0D5Qk00000TwmNCKAZ","500Qk00000K2hGhIAJ","TextPost","005Qk000001nVk9IAE","2024-12-30T16:24:36.000Z","Description: The root partition of Comerica's QA and PROD appliances is consistently 65-70% full. This ticket has been opened to address this issue to see if space can be increased or if the associated files taking up the most space can be stored somewhere else.","","","","005Qk000001nVk9IAE","2024-12-30T16:24:36.000Z"
"0D5Qk00000Twi6YKAR","500Qk00000K2hGhIAJ","TextPost","005Qk000001nVk9IAE","2024-12-30T16:25:12.000Z","Description Updated: The root partition of Comerica's QA and PROD appliances is consistently 65-70% full. This ticket has been opened to address this issue to see if space can be increased or if the associated files taking up the most space can be stored somewhere else.

The associated incident opened on Comerica's side is INC0887393.","","","","005Qk000001nVk9IAE","2024-12-30T16:25:12.000Z"
"0D5Qk00000U1UkiKAF","500Qk00000K2hGhIAJ","TextPost","0054u0000093L0nAAE","2024-12-31T15:36:18.000Z","What We've Learned Updated: There is a banner that will be displayed in the management console when the root partition on the epp appliance reaches 65% capacity. In every instance I've seen with Comerica, this is due to either their php logs or system logs getting a bit larger than expected. According to our engineering team, there is an automated job that will clean these up eventually and we've never seen capacity go past ~70%. We can go in and manually clean them as well, but that requires change controls on their side to be approved. As a side note, this should never impact server performance.","","","","0054u0000093L0nAAE","2024-12-31T15:36:18.000Z"
"0D5Qk00000U1eNcKAJ","500Qk00000K2hGhIAJ","TextPost","0054u0000093L0nAAE","2024-12-31T15:43:08.000Z","Next Steps Updated: To resolve the issue, we could upgrade their appliance to Ubuntu 22 ( it would most likely be handled by our devops team). This would increase the root partition to 50 gb from 20. However, we're going to need them to upgrade their appliance again in feb/march for something else so engineering is recommending we wait until then. A recommendation we can make until then could be to change the threshold at which it shows since there is no meaningful server impact anyways, but we'll need to wait until dev is back in office for that.","","","","0054u0000093L0nAAE","2024-12-31T15:43:08.000Z"
"0D5Qk00000U1sLJKAZ","500Qk00000K2hGhIAJ","TextPost","0054u0000093L0nAAE","2024-12-31T15:48:05.000Z","What We've Learned Updated: Andrew Siemasko met with Carl Saiyed, Lisa Wright, and Tamara Johnson yesterday about this ticket.  Andrew found that there is a banner that will be displayed in the management console when the root partition on the epp appliance reaches 65% capacity. In every instance I've seen with Comerica, this is due to either their php logs or system logs getting a bit larger than expected. According to our engineering team, there is an automated job that will clean these up eventually and we've never seen capacity go past ~70%. We can go in and manually clean them as well, but that requires change controls on their side to be approved. As a side note, this should never impact server performance.","","","","0054u0000093L0nAAE","2024-12-31T15:48:05.000Z"
"0D5Qk00000U1soOKAR","500Qk00000K2hGhIAJ","TextPost","0054u0000093L0nAAE","2024-12-31T15:55:36.000Z","Next Steps Updated: To solve this issue today Andrew asked Lisa if he can remove manually or if a change control is required (which it most likely will be) to clean up the instance, but he hasn't heard back as yet.

To permanently resolve the issue, we could upgrade their appliance to Ubuntu 22 ( it would most likely be handled by our devops team). This would increase the root partition to 50 gb from 20. However, we're going to need them to upgrade their appliance again in feb/march for something else so engineering is recommending we wait until then. A recommendation we can make until then could be to change the threshold at which it shows since there is no meaningful server impact anyways, but we'll need to wait until dev is back in office for that.","","","","0054u0000093L0nAAE","2024-12-31T15:55:36.000Z"
"0D5Qk00000Vc6twKAB","500Qk00000K2hGhIAJ","TextPost","0054u0000093L0nAAE","2025-01-27T21:00:23.000Z","<p>New email :</p><p> </p><p><b>From: </b>De La Garza, Maria J </p><p> <b>Date: </b>Monday, January 27, 2025 at 2:47 PM</p><p> </p><p> <b>Subject: </b>RE: Netwrix Support Ticket #00430904 - [Comerica] Root Partition&#39;s Disk Space is Consistently Nearing Capacity - ref:_00D7000000091pB._500Qk00000K2hGh:ref</p><p>Team,</p><p> </p><p> </p><p>The disk space issue is now showing 29% of available disk space. My understanding is that the job automatically kicks in at 70% of utilization. Does this mean the job hasn’t kicked in? or that the job didn’t clean up as expected? </p>","","","","0054u0000093L0nAAE","2025-01-27T21:00:23.000Z"
